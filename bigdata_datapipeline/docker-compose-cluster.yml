# define some variable
x-superset-image: 
  &superset-image apache/superset:${TAG:-latest-dev}
x-superset-depends-on: 
  &superset-depends-on
  - db
  - redis
x-superset-volumes:
  &superset-volumes # /app/pythonpath_docker will be appended to the PYTHONPATH in the final container
  - ./superset/docker:/app/docker
  - ./app_data/superset_home:/app/superset_home

version: "2"

# name: test_big_data_system

services:
  kafka-0:
    image: docker.io/bitnami/kafka:3.6
    container_name: kafka-0
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    ports:
      - "9092"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    volumes:
      - ./app_data/kafka_0_data:/bitnami/kafka

  kafka-1:
    image: docker.io/bitnami/kafka:3.6
    container_name: kafka-1
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    ports:
      - "9092"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    volumes:
      - ./app_data/kafka_1_data:/bitnami/kafka

  kafka-2:
    image: docker.io/bitnami/kafka:3.6
    container_name: kafka-2
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    ports:
      - "9092"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=2
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    volumes:
      - ./app_data/kafka_2_data:/bitnami/kafka

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.2
    container_name: schema-registry
    ports:
      - "8081:8081"
    depends_on:
      - kafka-0
      - kafka-1
      - kafka-2
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092

  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.2
    container_name: kafka-connect
    ports:
      - 8083:8083
    depends_on:
      - kafka-0
      - kafka-1
      - kafka-2
      - schema-registry
    volumes:
      - "./connector/secrets:/secrets"
      - "./connector/extra_plugins:/opt/connectors"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-0:9092,kafka-1:9092,kafka-2:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: _connect_configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: _connect_offset
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: _connect_status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      # converter
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      
      # security
      CONNECT_CONFIG_PROVIDERS: file
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: org.apache.kafka.common.config.provider.FileConfigProvider

      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/opt/connectors
    command:
      - bash
      - -c
      - |
        echo "Installing Connector"
        confluent-hub install --no-prompt debezium/debezium-connector-mysql:1.7.0
        confluent-hub install --no-prompt debezium/debezium-connector-mongodb:2.1.4
        confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:1.11.0
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity

  kafka-ui:
    image: provectuslabs/kafka-ui:master    # latest also ok
    container_name: kafka-ui
    ports:
      - 8888:8080 # Changed to avoid port clash with akhq
    depends_on:
      - kafka-0
      - kafka-1
      - kafka-2
      - schema-registry
      - kafka-connect
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-0:9092, kafka-1:9092, kafka-2:9092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
      DYNAMIC_CONFIG_ENABLED: 'true'

  mysql:
    # *-----------------------------*
    # To connect to the DB:
    #   docker exec -it mysql bash -c 'mysql -u root -p$MYSQL_ROOT_PASSWORD demo'
    # *-----------------------------*
    image: mysql:8.2
    container_name: mysql
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=debezium
      - MYSQL_USER=mysqluser
      - MYSQL_PASSWORD=mysqlpw
      # - ALLOW_EMPTY_PASSWORD=yes
    volumes:
      - ./init_data/mysql/initialization:/docker-entrypoint-initdb.d
      - ./init_data/mysql:/data

  kafkacat:   # use like kafka consumer
    image: edenhill/kafkacat:1.6.0
    container_name: kafkacat
    depends_on:
      - kafka-0
      - kafka-1
      - kafka-2
      - schema-registry
      - kafka-connect
    entrypoint: 
      - /bin/sh 
      - -c 
      - |
        apk add jq; 
        while [ 1 -eq 1 ];do sleep 60;done

  spark-master-1:   # to read kafka stream and write to HDFS
    build: 
      context: ./spark_setup
    image: my_spark_3.5.0:v0
    container_name: spark-master-1
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - 18085:18085
      - 17077:17077
    environment:
      SPARK_MASTER_PORT: 17077
      SPARK_MASTER_WEBUI_PORT: 18085
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
    volumes:
      # - ./:/home
      - ./spark_submits:/opt/bitnami/spark/work:z

  spark-worker-11:
    build: 
      context: ./spark_setup
    image: my_spark_3.5.0:v0
    container_name: spark-worker-11
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master-1:17077
    depends_on:
      - spark-master-1
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master-1:17077
      # SPARK_WORKER_PORT: 28085
    # volumes:
    #   - ./spark_submits:/opt/bitnami/spark/work:z   # to use spark-submit with csv file https://github.com/bitnami/containers/issues/44705#issuecomment-1696802976

  spark-worker-12:
    build: 
      context: ./spark_setup
    image: my_spark_3.5.0:v0
    container_name: spark-worker-12
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master-1:17077
    depends_on:
      - spark-master-1
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master-1:17077
      # SPARK_WORKER_PORT: 28085
    # volumes:
    #   - ./spark_submits:/opt/bitnami/spark/work:z   # to use spark-submit with csv file https://github.com/bitnami/containers/issues/44705#issuecomment-1696802976

  # nifi+zookeeper
  zookeeper:
    hostname: zookeeper
    container_name: zookeeper
    image: 'bitnami/zookeeper:3.9'
    ports:
      - 2181:2181
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  nifi01:
    image: apache/nifi:1.23.2
    container_name: nifi01
    user: root
    ports:
      - 8080:8080 # change port for each nifi if config with multiple nifis
    depends_on:
      - spark-master-1
      - spark-worker-11
      - spark-worker-12
      - kafka-0
      - kafka-1
      - kafka-2
      - schema-registry
      - kafka-connect
      - mysql
    volumes:
      - ./app_data/nifi:/opt/nifi/nifi-current/data
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_CLUSTER_IS_NODE=false    # if cluster then set true
      - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
      - NIFI_ZK_CONNECT_STRING=zookeeper:2181
      - NIFI_ELECTION_MAX_WAIT=1 min
      - NIFI_SENSITIVE_PROPS_KEY=MySuperSecretKey # Set your sensitive properties key here
  # nifi02:
  #   image: apache/nifi:latest
  #   ports:
  #     - 6979:8080
  #   environment:
  #     - NIFI_WEB_HTTP_PORT=8080
  #     - NIFI_CLUSTER_IS_NODE=true
  #     - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
  #     - NIFI_ZK_CONNECT_STRING=zookeeper:2181
  #     - NIFI_ELECTION_MAX_WAIT=1 min
  #     - NIFI_SENSITIVE_PROPS_KEY=MySuperSecretKey # Set your sensitive properties key here
  # nifi03:
  #   image: apache/nifi:latest
  #   ports:
  #     - 6978:8080
  #   environment:
  #     - NIFI_WEB_HTTP_PORT=8080
  #     - NIFI_CLUSTER_IS_NODE=true
  #     - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
  #     - NIFI_ZK_CONNECT_STRING=zookeeper:2181
  #     - NIFI_ELECTION_MAX_WAIT=1 min
  #     - NIFI_SENSITIVE_PROPS_KEY=MySuperSecretKey # Set your sensitive properties key here

# volumes:
#   kafka_0_data:
#     driver: local
#   kafka_1_data:
#     driver: local
#   kafka_2_data:
#     driver: local

  ######################## test hadoop integration ########################
  namenode:
    build: 
      context: ./hadoop_setup/namenode
    image: docker.io/library/my_hadoop_3.2.4:namenode 
    container_name: namenode
    working_dir: /root
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - ./app_data/hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop_setup/hadoop.env

  datanode1:
    build: 
      context: ./hadoop_setup/datanode
    image: docker.io/library/my_hadoop_3.2.4:datanode 
    container_name: datanode1
    restart: always
    volumes:
      - ./app_data/hadoop_datanode1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop_setup/hadoop.env

  resourcemanager:
    build: 
      context: ./hadoop_setup/resourcemanager
    image: docker.io/library/my_hadoop_3.2.4:resourcemanager 
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864"
    env_file:
      - ./hadoop_setup/hadoop.env

  nodemanager1:
    build: 
      context: ./hadoop_setup/nodemanager
    image: docker.io/library/my_hadoop_3.2.4:nodemanager 
    container_name: nodemanager1
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864 resourcemanager:8088"
    env_file:
      - ./hadoop_setup/hadoop.env
  
  historyserver:
    build: 
      context: ./hadoop_setup/historyserver
    image: docker.io/library/my_hadoop_3.2.4:historyserver 
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864 resourcemanager:8088"
    volumes:
      - ./app_data/hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop_setup/hadoop.env
  
  # hive-server:
  #   image: bde2020/hive:2.3.2-postgresql-metastore
  #   container_name: hive-server
  #   depends_on:
  #     - namenode
  #     - datanode1
  #   env_file:
  #     - ./hadoop-hive.env
  #   environment:
  #     HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
  #     SERVICE_PRECONDITION: "hive-metastore:9083"
  #   ports:
  #     - "10000:10000"

  # hive-metastore:
  #   image: bde2020/hive:2.3.2-postgresql-metastore
  #   container_name: hive-metastore
  #   env_file:
  #     - ./hadoop-hive.env
  #   command: /opt/hive/bin/hive --service metastore
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 hive-metastore-postgresql:5432"
  #   ports:
  #     - "9083:9083"

  # hive-metastore-postgresql:
  #   image: bde2020/hive-metastore-postgresql:2.3.0
  #   container_name: hive-metastore-postgresql

  # presto-coordinator:
  #   image: shawnzhu/prestodb:0.181
  #   container_name: presto-coordinator
  #   ports:
  #     - "8089:8089"

  spark-master-2:   # to read HDFS
    build: 
      context: ./spark_setup
    image: my_spark_3.5.0:v0
    container_name: spark-master-2
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - 28085:28085
      - 27077:27077
    environment:
      SPARK_MASTER_PORT: 27077
      SPARK_MASTER_WEBUI_PORT: 28085
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
    volumes:
      # - ./:/home
      - ./spark_submits:/opt/bitnami/spark/work:z

  spark-worker-21:
    build: 
      context: ./spark_setup
    image: my_spark_3.5.0:v0
    container_name: spark-worker-21
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master-2:27077
    depends_on:
      - spark-master-2
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master-2:27077

  spark-worker-22:
    build: 
      context: ./spark_setup
    image: my_spark_3.5.0:v0
    container_name: spark-worker-22
    user: root  # solved permission, link: https://askubuntu.com/a/1355230/1661448
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master-2:27077
    depends_on:
      - spark-master-2
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master-2:27077

  # postgres: data mart
  postgresql-datamart:
    image: docker.io/bitnami/postgresql:16
    container_name: postgres-datamart
    user: root
    ports:
      - '5432:5432'   # avoid duplicate with superset-postgres
    volumes:
      - './app_data/postgresql_datamart:/bitnami/postgresql'
      - './init_data/postgres_datamart/init.sql:/docker-entrypoint-initdb.d/init.sql'  # Mount the SQL script as an initialization script
    environment:
      - POSTGRESQL_USERNAME=my_user
      - POSTGRESQL_PASSWORD=password123
      - POSTGRESQL_DATABASE=homecam_database
    # extra_hosts:  # https://stackoverflow.com/a/61001152/18448121
    #   docker.host: 172.17.0.1

  redis:
    image: redis:7
    container_name: superset_cache
    restart: unless-stopped
    volumes:
      - ./app_data/superset_redis:/data

  db:
    env_file: ./superset/docker/.env-non-dev
    image: postgres:15
    container_name: superset_db
    restart: unless-stopped
    volumes:
      - ./app_data/superset_db_home:/var/lib/postgresql/data
      - ./superset/docker/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d

  superset:
    env_file: ./superset/docker/.env-non-dev
    image: *superset-image
    container_name: superset_app
    command: ["/app/docker/docker-bootstrap.sh", "app-gunicorn"]
    user: "root"
    restart: unless-stopped
    ports:
      - 8089:8088
    depends_on: *superset-depends-on
    volumes: *superset-volumes

  superset-init:
    image: *superset-image
    container_name: superset_init
    command: ["/app/docker/docker-init.sh"]
    env_file: ./superset/docker/.env-non-dev
    depends_on: *superset-depends-on
    user: "root"
    volumes: *superset-volumes
    healthcheck:
      disable: true

  superset-worker:
    image: *superset-image
    container_name: superset_worker
    command: ["/app/docker/docker-bootstrap.sh", "worker"]
    env_file: ./superset/docker/.env-non-dev
    restart: unless-stopped
    depends_on: *superset-depends-on
    user: "root"
    volumes: *superset-volumes
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "celery -A superset.tasks.celery_app:app inspect ping -d celery@$$HOSTNAME",
        ]

  superset-worker-beat:
    image: *superset-image
    container_name: superset_worker_beat
    command: ["/app/docker/docker-bootstrap.sh", "beat"]
    env_file: ./superset/docker/.env-non-dev
    restart: unless-stopped
    depends_on: *superset-depends-on
    user: "root"
    volumes: *superset-volumes
    healthcheck:
      disable: true

  # mongo data source
  mongodb-primary:  # ok (VMWare)
    image: docker.io/bitnami/mongodb:5.0.23
    container_name: mongo1
    user: root
    ports:
      - 27017:27017
    environment:
      # access "data" database only account
      # access via mongosh: mongosh --host mongo1:27017 -u my_user -p password123 --authenticationDatabase admin
      # - MONGODB_USERNAME=my_user
      # - MONGODB_PASSWORD=password123
      # - MONGODB_DATABASE=data

      # access all databases
      # access via mongosh: mongosh --host mongo1:27017 -u root -p password123
      - MONGODB_ROOT_USER=root
      - MONGODB_ROOT_PASSWORD=password123

      # set replica
      - MONGODB_ADVERTISED_HOSTNAME=mongo1
      - MONGODB_INITIAL_PRIMARY_HOST=mongo1
      - MONGODB_REPLICA_SET_MODE=primary
      - MONGODB_REPLICA_SET_NAME=rs0
      - MONGODB_REPLICA_SET_KEY=abcdefghijklmno

    volumes:
      - './app_data/mongo/mongo1:/bitnami/mongodb'

  mongodb-secondary1:
    image: docker.io/bitnami/mongodb:5.0.23
    container_name: mongo2
    user: root
    depends_on:
      - mongodb-primary
    environment:
      - MONGODB_REPLICA_SET_NAME=rs0
      - MONGODB_REPLICA_SET_MODE=secondary
      - MONGODB_ADVERTISED_HOSTNAME=mongo2
      - MONGODB_INITIAL_PRIMARY_HOST=mongo1
      - MONGODB_INITIAL_PRIMARY_PORT_NUMBER=27017
      - MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD=password123
      - MONGODB_REPLICA_SET_KEY=abcdefghijklmno
    volumes:
      - './app_data/mongo/mongo2:/bitnami/mongodb'

  # mongodb-secondary2:
  #   image: docker.io/bitnami/mongodb:5.0.23
  #   container_name: mongo3
  #   user: root
  #   depends_on:
  #     - mongodb-primary
  #   environment:
  #     - MONGODB_REPLICA_SET_NAME=rs0
  #     - MONGODB_REPLICA_SET_MODE=secondary
  #     - MONGODB_ADVERTISED_HOSTNAME=mongo3
  #     - MONGODB_INITIAL_PRIMARY_HOST=mongo1
  #     - MONGODB_INITIAL_PRIMARY_PORT_NUMBER=27017
  #     - MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD=password123
  #     - MONGODB_REPLICA_SET_KEY=abcdefghijklmno
  #   volumes:
  #     - './app_data/mongo/mongo3:/bitnami/mongodb'


  # mongo1:
  #   container_name: mongo1
  #   image: mongo:5.0.23
  #   ports:
  #     - 20000:27017
  #   volumes:
  #     - ./app_data/mongo/mongo1:/data/db
  #     - ./init_data/mongo/config/mongod.conf:/etc/mongod.conf
  #     # - ./rs-init.sh:/scripts/rs-init.sh
  #   restart: always
  #   entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "dbrs" ]
  #   environment:
  #     - MONGO_INITDB_ROOT_USERNAME=admin
  #     - MONGO_INITDB_ROOT_PASSWORD=123456
 
  # mongo2:
  #   container_name: mongo2
  #   image: mongo:5.0.23
  #   ports:
  #     - 20001:27017
  #   depends_on:
  #     - mongo1
  #   volumes:
  #     - ./app_data/mongo/mongo2:/data/db
  #   restart: always
  #   entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "dbrs" ]
 
  # mongo3:
  #   container_name: mongo3
  #   image: mongo:5.0.23
  #   ports:
  #     - 20002:27017
  #   depends_on:
  #     - mongo1     
  #   volumes:
  #     - ./app_data/mongo/mongo3:/data/db
  #   restart: always
  #   entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "dbrs" ]


  # postgres data source
  timescaledb:
    image: my_custom_image/timescaledb:2.4
    container_name: timescaledb
    hostname: timescaledb
    build:
      context: ./init_data/timescaledb
    ports:
     - 5433:5432
    environment:
     - POSTGRES_USER=postgres
     - POSTGRES_PASSWORD=postgres

  